---
title: "HW3 - Exercise3"
author: JooChul Lee
date: "`r format(Sys.time(), '%d %B %Y')`"
documentclass: article
knit: "bookdown::render_book('Exercise2.Rmd', 'bookdown::pdf_document2')"
fontsize: 11pt
papersize: letter
abstract: This is the . 
---


```{r setup, echo = FALSE, message = FALSE, warning = FALSE}

## for latex and html output
isHtml <- knitr::is_html_output()
isLatex <- knitr::is_latex_output()
latex <- ifelse(isLatex, '\\LaTeX\\', 'LaTeX')

## specify global chunk options
knitr::opts_chunk$set(fig.width = 5, fig.height = 4, dpi = 300,
                      out.width = "90%", fig.align = "center")

```

# Many local maxima 

## Find the the log-likelihood function and plot it.
log-likelihood function of $\theta$ :

$$ l(\theta) = -n\ln2\pi+\sum_{i=1}^n\ln[1-\cos(x_i-\theta)]$$

```{r, echo=FALSE}
Data = c(3.91, 4.85, 2.28, 4.06, 3.70, 4.04, 5.46, 3.53, 2.28, 1.96, 2.53, 3.88, 2.22, 3.47, 4.82, 2.46, 2.99, 2.54, 0.52)
loglik = function(theta) 
{
   sum( log(1 - cos(Data - theta)) ) - length(Data)*log(2*pi)
}
curve(sapply(x, loglik), -pi, pi, xlab = expression(theta), ylab = 'loglikleyhood')
```


## Find the method-of-moments estimator

\begin{align}
    E(X) &=  \int^{2\pi}_0 \dfrac{1 - cos(x-\theta)}{2\pi}x dx \\
         &= \int^{2\pi}_0 \dfrac{x}{2\pi} dx - \int^{2\pi}_0 x cos(x-\theta)dx \\
         &=\frac{1}{2\pi}(2\pi^2 + 2\pi sin \theta) \\
         &= \pi - \frac{1}{2\pi}(-2\pi sin(\theta)) \\
         &= \pi + sin(\theta)
\end{align}

Thus, we can get 
$$\hat{\theta} = sin^{-1}(\bar{X} - \pi)$$

Thus,
$$ \hat{\theta} = 0.0953 \; or \; 3.046 $$

## Find the MLE using the Newtonâ€“Raphson method

This is the code for table 1.

```{r}
sample <- c(3.91, 4.85, 2.28, 4.06, 3.70, 4.04, 5.46, 3.53, 2.28, 1.96, 
            2.53, 3.88, 2.22, 3.47, 4.82, 2.46, 2.99, 2.54, 0.52)
F_S_D = function(theta)
{
   First = sum( -sin(sample - theta)/(1-cos(sample - theta)) ) 
   Second = sum( 1/(cos(sample - theta) - 1 ) ) 
   list(First = First, Second= Second )
}
        
N_R = function(initial, max = 100, tol = 1e-5)
{  
   current = initial
   for(i in 1:max)
   {
      new = current - F_S_D(current)$First/F_S_D(current)$Second
      if(abs(new -current) < tol) break
      current1 = current
      current = new
   }   
   return( c(current, i, diff = abs(current -current1) ) )
}
N_R(0.0953941)[1]
N_R(3.046199)[1]
```

When the initial value is 0.0953941, the mle is 0.003118157.
When the initial value is 3.046199, the mle is 3.170713.

## What solutions do you find when you start at $\theta_0 = -2.7$ and $\theta_0 = 2.7$

```{r}
sample <- c(3.91, 4.85, 2.28, 4.06, 3.70, 4.04, 5.46, 3.53, 2.28, 1.96, 
            2.53, 3.88, 2.22, 3.47, 4.82, 2.46, 2.99, 2.54, 0.52)
F_S_D = function(theta)
{
   First = sum( -sin(sample - theta)/(1-cos(sample - theta)) ) 
   Second = sum( 1/(cos(sample - theta) - 1 ) ) 
   list(First = First, Second= Second )
}
        
N_R = function(initial, max = 100, tol = 1e-5)
{  
   current = initial
   for(i in 1:max)
   {
      new = current - F_S_D(current)$First/F_S_D(current)$Second
      if(abs(new -current) < tol) break
      current1 = current
      current = new
   }   
   return( c(current, i, diff = abs(current -current1) ) )
}
N_R(-2.7)[1]
N_R(2.7)[1]
```

When the initial value is -2.7, the mle is -2.668857
When the initial value is 2.7, the mle is 2.848423

## Repeat the above using 200 equally spaced starting values 
```{r}
sample <- c(3.91, 4.85, 2.28, 4.06, 3.70, 4.04, 5.46, 3.53, 2.28, 1.96, 
            2.53, 3.88, 2.22, 3.47, 4.82, 2.46, 2.99, 2.54, 0.52)
F_S_D = function(theta)
{
   First = sum( -sin(sample - theta)/(1-cos(sample - theta)) ) 
   Second = sum( 1/(cos(sample - theta) - 1 ) ) 
   list(First = First, Second= Second )
}
        
N_R = function(initial, max = 100, tol = 1e-5)
{  
   current = initial
   for(i in 1:max)
   {
      new = current - F_S_D(current)$First/F_S_D(current)$Second
      if(abs(new -current) < tol) break
      current1 = current
      current = new
   }   
   return( c(current, i, diff = abs(current -current1) ) )
}
initial = seq(-pi, pi, length.out = 200)
result = matrix(0,2, 200)
result[1,] =initial
for(i in 1:200)
   result[2,i] = N_R(initial[i],tol = 1e-6)[1]
rownames(result) <- c("Initial Value", "Root")

knitr::kable(result[,1:8] ,booktabs = TRUE, align='c')
knitr::kable(result[,9:16] ,booktabs = TRUE, align='c')
knitr::kable(result[,17:24] ,booktabs = TRUE, align='c')
knitr::kable(result[,25:32] ,booktabs = TRUE, align='c')
knitr::kable(result[,33:40] ,booktabs = TRUE, align='c')
knitr::kable(result[,41:48] ,booktabs = TRUE, align='c')
knitr::kable(result[,29:56] ,booktabs = TRUE, align='c')
knitr::kable(result[,57:64] ,booktabs = TRUE, align='c')
knitr::kable(result[,65:72] ,booktabs = TRUE, align='c')
knitr::kable(result[,73:80] ,booktabs = TRUE, align='c')
knitr::kable(result[,81:88] ,booktabs = TRUE, align='c')
knitr::kable(result[,89:96] ,booktabs = TRUE, align='c')
knitr::kable(result[,97:104] ,booktabs = TRUE, align='c')
knitr::kable(result[,105:112] ,booktabs = TRUE, align='c')
knitr::kable(result[,113:120] ,booktabs = TRUE, align='c')
knitr::kable(result[,121:128] ,booktabs = TRUE, align='c')
knitr::kable(result[,129:136] ,booktabs = TRUE, align='c')
knitr::kable(result[,137:144] ,booktabs = TRUE, align='c')
knitr::kable(result[,145:152] ,booktabs = TRUE, align='c')
knitr::kable(result[,153:160] ,booktabs = TRUE, align='c')
knitr::kable(result[,161:168] ,booktabs = TRUE, align='c')
knitr::kable(result[,169:176] ,booktabs = TRUE, align='c')
knitr::kable(result[,177:184] ,booktabs = TRUE, align='c')
knitr::kable(result[,185:192] ,booktabs = TRUE, align='c')
knitr::kable(result[,193:200] ,booktabs = TRUE, align='c')
plot(seq(-pi, pi, length.out = 200), result[2,], xlab="Initial Values", ylab="Root", type="b")
DATA <- as.data.frame(round(result[2,], digits = 4))
colnames(DATA) = c('Root')
uniq <- unique(DATA)
knitr::kable(uniq)
```

\newpage

# Modeling beetle data

## Show the contour plot of the sum of squared errors AND Gauss-Newton method
```{r}
library(plotly)
Data <- data.frame(days = c(0, 8, 28, 41, 63, 69, 97, 117, 135, 154), beetles = c(2, 47, 192, 256, 768, 896, 1120, 896, 1184, 1024))
t <- Data$days
b <- Data$beetles
N0 <- b[1]
ct <- function(k, r)
{
   sum((k * (N0) / (N0 + (k - N0) * exp(-r * t)) - b)^2)
}
k <- seq(0, 2000, length.out = 1e4)
r <- seq(0, 0.5, length.out = 1e2) 
z <- outer(k,r, Vectorize(ct) )
contour(k, r, z, xlab = "k", ylab = "r", main = "contour plot")
```

```{r}
beetles <- data.frame(
   days = c(0, 8, 28, 41, 63, 69, 97, 117, 135, 154),
   beetles = c(2, 47, 192, 256, 768, 896, 1120, 896, 1184, 1024))

errors <- rep(NA, nrow(beetles))

nls(beetles ~ (K*beetles[1])/(beetles[1]+(K-beetles[1])*exp(-r*days)), 
    start = list(K = 1000, r = 1), data = beetles, trace = TRUE)
```
By Gauss-Newton method, I can get 1049.4068 and 0.1183 for K and r

## Find the maximum likelihood estimators and Estimate the variance your parameter estimates

```{r}
rm(list=ls())
l <- expression(
  log(1/(sqrt(2*pi)*sigma))-(log((2*2+2*(K-2)*exp(-r*0))/(2*K)))^2/(2*sigma^2)+
  log(1/(sqrt(2*pi)*sigma))-(log((2*47+47*(K-2)*exp(-r*8))/(2*K)))^2/(2*sigma^2)+
  log(1/(sqrt(2*pi)*sigma))-(log((2*192+192*(K-2)*exp(-r*28))/(2*K)))^2/(2*sigma^2)+
  log(1/(sqrt(2*pi)*sigma))-(log((2*256+256*(K-2)*exp(-r*41))/(2*K)))^2/(2*sigma^2)+
  log(1/(sqrt(2*pi)*sigma))-(log((2*768+768*(K-2)*exp(-r*63))/(2*K)))^2/(2*sigma^2)+
  log(1/(sqrt(2*pi)*sigma))-(log((2*896+896*(K-2)*exp(-r*69))/(2*K)))^2/(2*sigma^2)+
  log(1/(sqrt(2*pi)*sigma))-(log((2*1120+1120*(K-2)*exp(-r*97))/(2*K)))^2/(2*sigma^2)+
  log(1/(sqrt(2*pi)*sigma))-(log((2*896+896*(K-2)*exp(-r*117))/(2*K)))^2/(2*sigma^2)+
  log(1/(sqrt(2*pi)*sigma))-(log((2*1184+1184*(K-2)*exp(-r*135))/(2*K)))^2/(2*sigma^2)+
  log(1/(sqrt(2*pi)*sigma))-(log((2*1024+1024*(K-2)*exp(-r*154))/(2*K)))^2/(2*sigma^2))

lpk <- D(l,"K")
lpr <- D(l,"r")
lps <- D(l,"sigma")
lppkk <- D(D(l,"K"),"K")
lppkr <- D(D(l,"K"),"r")
lppks <- D(D(l,"K"),"sigma")
lpprr <- D(D(l,"r"),"r")
lpprs <- D(D(l,"r"),"sigma")
lppss <- D(D(l,"sigma"),"sigma")
count <- 0
process <- TRUE
krs <- matrix(c(1050, 0.12, 0.5))
while(process){
  K <- krs[1]
  r <- krs[2]
  sigma <- krs[3]
  gp <- matrix(c(eval(lpk), eval(lpr), eval(lps)))
  gpt <- t(gp)
  M <- matrix(c(eval(lppkk),eval(lppkr),eval(lppks),eval(lppkr),eval(lpprr),
                eval(lpprs),eval(lppks),eval(lpprs),eval(lppss)),byrow=TRUE,nrow=3)
  Minv <- solve(M)
  krs <-  krs - Minv %*% gp
  count <- count + 1
  if(gpt%*%gp < 1e-6 | count == 1000)
    process = FALSE
}
count
krss <- matrix(c(K,r,sigma^2), ncol = 3)
colnames(krss) <- c("K", "r", "sigma2")
knitr::kable(krss)
vari <- solve(-M)
colnames(vari) <- row.names(vari) <- c("K", "r", "sigma")
knitr::kable(vari)
```
